[data]
out_folder: name of the output folder where experiments will be saved

tr_fea_scp: list of the files containing the training features
tr_fea_opts: kaldi command for feature processing (e.g. normalizations, add-delta features)
tr_lab_folder: folder where training alignments (labels) have been produced by kaldi
tr_lab_opts: label type (use ali-to-pdf for standard phone-states labels)

dev_fea_scp: list of the files containing the dev features (used for cross-validation)
dev_fea_opts: kaldi command for feature processing (should be the same used for training)
dev_lab_folder: folder where dev alignments (labels) have been produced by kaldi
dev_lab_opts:  label type (should be the same used for training)


te_fea_scp: list of the files containing the test features
te_fea_opts: kaldi command for feature processing (should be the same used for training)
te_lab_folder: folder where test alignments (labels) have been produced by kaldi
te_lab_opts: label type (should be the same used for training)


[architecture]
NN_type: type of neural network (it could be MLP, RNN, LSTM, GRU)
cnn_pre: If set to one processes input feature with a two layer cnn. It is currently supported for RNN models with cw_left=5 and cw_right=5.
cnn_type: If this option is set to CNN_on_cw, the CNN will be applied on each context window of the input(frames in window vs feats). If thie option is set to CNN_on_batch, CNN will be applied on each input batch(frames(context window) in batch vs feats). If nothing is given, it will be set to CNN_on_cw in default.
cnn_filter_size: size of the CNN layer's kernel with default value 3
cnn_paddings: number of 0 paddings with default value 1
hidden_dim: number of hidden neurons
N_hid: number of hidden layers
drop_rate: dropout rate (it will be standard dropout for MLP, recurrent dropout for RNN models)
use_batchnorm: use batch norm for feed-forward connections
use_laynorm: use layer norm
cw_left: number of past frames of the context window
cw_right: number of future frames of the context window
seed: random seed used for initialization
use_cuda: if set to 1 uses cuda (strongly suggested)
bidir: using bidirectional RNNs (if NN_type=MLP this options is neglected)
resnet: when set to 1 uses feed-forward resnet skip-connections
skip_conn: if set to 1 uses skip-connections between two consecutive linear transformation
act: activation function (sigmoid,tanh,relu)
act_gate: activation function for multiplicative gates (only for RNNs models)
resgate: if set to 1 adds reset gate (only for GRU model)
minimal_gru: if set to 1 uses minimal GRU setting (only for GRU model)
cost:cost function (nll,mse)
twin_reg: if set to 1 enables twin regularization (for RNN models only)
twin_w: weight factor for twin regularization (for RNN models only)
multi_gpu: if set to 1 exploits it performs data parallelization over all the GPUs available

[optimization]
N_ep: number of training epochs
lr: initial learning rate
optimizer: type of optimizer (rmsprop,sgd)
halving_factor: halving factor for learning rate decay
improvement_threshold: multiplies the current learning rate with the given halving factor  when the  relative improvement is less than improvement_threshold
batch_size: batch size during training
save_gpumem: with save_gpumem=1 it does not save all the input data into GPU memory

[decoding]
count_file: count files produced by kaldi (it contains counts for each phone state. It is used for normalizing the posterior probabilities generated by the DNN)
graph_dir: graph generated by kaldi (it should be the same used for producing the alignments)
data_dir: kaldi data folder (useful for text-level labels)
ali_dir:kaldi alignments folder


